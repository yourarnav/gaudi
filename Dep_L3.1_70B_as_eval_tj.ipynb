{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5f80b8",
   "metadata": {},
   "source": [
    "### Init vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Specify the model ID and number of GPUs\n",
    "model_id = \"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\"\n",
    "number_gpus = 2\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES to specify GPUs 5 and 7\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,7\"\n",
    "\n",
    "# Load the model using vLLM\n",
    "model = LLM(model=model_id, tensor_parallel_size=number_gpus)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set up the sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.2,\n",
    "    max_tokens=6500,\n",
    "    n=100  # Number of generations per prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f7a87",
   "metadata": {},
   "source": [
    "# Mistral 3 summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c89cf",
   "metadata": {},
   "source": [
    "## Aspect Covergae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e73ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m3_50.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "    \n",
    "aspect_coverage_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating. You will then be given one summary written for the set of information. Your task is to rate the summary on one metric. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Aspect Coverage - Aspect Coverage measures how completely a summary captures the major features, characteristics, or attributes of a product that are prominently discussed in the original product information. Summaries should be penalized for missing any major aspects and rewarded for covering all important aspects thoroughly.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - Summary does not cover any important aspects present in the set of information.\n",
    "<score>2</score> - Summary does not cover most of the important aspects present in the set of information.\n",
    "<score>3</score> - Summary covers around half of the important aspects present in the set of information.\n",
    "<score>4</score> - Summary covers most of the important aspects present in the set of information.\n",
    "<score>5</score> - Summary covers all the important aspects discussed in the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the important aspects present in the set of information and list them with numbering.\n",
    "2. Identify the important aspects present in the summary and list them with numbering.\n",
    "3. Identify the important aspects covered by the summary that are present in the set of information and list them with numbering.\n",
    "4. Calculate the total number of important aspects covered by the summary that are present in the set of information.\n",
    "5. Calculate the total number of important aspects present in the set of information.\n",
    "6. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation of how much is the coverage and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = aspect_coverage_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m3_ac_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14772c28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Fluency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4becdd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m3_50.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "fluency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Fluency : The quality of summary in terms of grammar, spelling, punctuation, capitalization, word choice, and sentence structure and should contain no errors. The summary should be easy to read, follow, comprehend and should contain no errors.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary is all garbled and does not make any sense.\n",
    "<score>2</score> - The summary has grammatical errors that make it hard to understand or sound unnatural.\n",
    "<score>3</score> - The summary has errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "<score>4</score> - The summary has very few errors, but it is easy to read, follow and comprehend.\n",
    "<score>5</score> - The summary is extremely fluent and is easy to read, follow and comprehend.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the sentences presented in the summary and list them with numbering.\n",
    "2. Go through each sentence and list down if there are any fluency problems.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on fluency of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = fluency_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m3_fl_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec3a4c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a675491",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m3_50.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "coherence_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Coherence - The collective quality of all sentences. The summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information.\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary lacks structure and logical flow, resulting in disjointed ideas and significant inconsistencies, making it confusing and challenging to follow.\n",
    "<score>2</score> - The summary attempts coherence but struggles with occasional lapses in logic, clarity issues, and insufficiently connected ideas, leading to a somewhat disjointed presentation.\n",
    "<score>3</score> - The summary displays a reasonable level of coherence with a logical sequence, yet occasional disruptions in flow and clarity, requiring some improvements for a smoother transition between ideas.\n",
    "<score>4</score> - The summary demonstrates strong coherence, maintaining a clear and organized flow with effective transitions and minimal inconsistencies, effectively conveying main points with clarity and precision.\n",
    "<score>5</score> - The summary showcases exceptional coherence with a flawless logical flow, impeccable transitions, and consistent clarity, presenting information in an impeccably organized and easily comprehensible manner.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary.\n",
    "2. Check if everything is presented in a clear and logical order. Give a clear step-by-step explanation of what you found and what is lacking.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation on coherence of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = coherence_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m3_co_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947727cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175e417",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m3_50.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "faithfulness_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Faithfulness - Faithfulness measures the extent to which every piece of information mentioned in the summary is verifiable, supported, present, or can be reasonably inferred from the input. The input includes product title, description, key features, specifications, reviews, and average rating. Summaries should be penalized if they contain information that cannot be verified from the provided input or if they make broad generalizations that are not supported by the input data.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "\n",
    "<score>1</score> - The summary is for a different product and is irrelevant/unrelated to the given set of information.\n",
    "<score>2</score> - The summary contains very few facts actually verifiable/supported/present/inferred from the set of information and contains a lot of hallucinated facts.\n",
    "<score>3</score> - The summary contains more than one piece of information that is not verifiable/present/inferred from the set of information.\n",
    "<score>4</score> - The summary contains only one piece of information that is not verifiable/supported/present/inferred from the the set of information.\n",
    "<score>5</score> - Every piece of information present in the summary is verifiable/supported/present/inferred from the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary that is not verifiable/supported/present/inferred from the set of information. Give a clear step-by-step explanation of what you found.\n",
    "2. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on faithfulness and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = faithfulness_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m3_fa_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702b547",
   "metadata": {},
   "source": [
    "# Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f10235",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m3_50.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "relevance_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Relevance - Relevance measures the selection of important information from the input, including product title, description, key features, specifications, reviews, and average rating. The summary should include only important and relevant information from the input. Summaries should not contain redundancies or excess information.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary misses all the important opinions majorly discussed in the set of information.\n",
    "<score>2</score> - The summary misses most of the important opinions majorly discussed in the set of information or mostly has redundant/excess/unimportant details\n",
    "<score>3</score> - The summary covers around half of the important opinions majorly discussed in the set of information. or contains redundant/excess/unimportant details.\n",
    "<score>4</score> - The summary covers most of the important opinions majorly discussed in the set of information and has very less amount of redundant/excess/unimportant details.\n",
    "<score>5</score> - The summary covers all the important opinions majorly discussed in the set of information and has no redundant/excess/unimportant details.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify all the important opinions majorly discussed in the set of information and list them with numbering.\n",
    "2. Identify the important opinions present in the summary and list them with numbering.\n",
    "3. Next identify how many important opinions are present in both summary and the set of information and list them with numbering\n",
    "4. Next idenify the how many redundant/excess/unimportant details does the summary have and list them with numbering.\n",
    "5. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on relevance and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = relevance_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m3_re_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a1040",
   "metadata": {},
   "source": [
    "# Sentiment Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d435f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "\n",
    "with open(\"m3_50.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "sentiment_consistency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description: \n",
    "You will be given a set of information such as reviews, and average rating and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - None of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>2</score> - Very few of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>3</score> - Only around half of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>4</score> - Most of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>5</score> - All aspects present in summary have the same majority sentiment as in reviews.\n",
    "\n",
    "\n",
    "Product Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary to Evaluate: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the aspects and their sentiment present in the summary and list them with numbering.\n",
    "2. For the list of aspects identified, identify the majority sentiment from the reviews and list them with numbering.\n",
    "3. Next identify how many aspect and sentiment match between reviews and summary from above and list them with numbering.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on sentiment preservation of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "\n",
    "def evaluate_opinion_summary(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "\n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**Product {i + j + 1} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "\n",
    "with open(\"m3_sc_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e56c69f",
   "metadata": {},
   "source": [
    "# Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611eb5cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m3_50.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "specificity_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Generic Opinion example: The battery is good.\n",
    "Specific Opinion example: The battery lasts for more than 12 hours on a single charge.\n",
    "\n",
    "Specificity - Specificity measures the level of detail and precision in the information and opinions presented in the summary. A specific summary provides concrete facts, measurements, or detailed descriptions about the product's features, performance, and user experiences. It avoids vague or general statements and instead offers precise information that gives readers a clear and thorough understanding of the product's characteristics and performance. \n",
    "\n",
    "Summaries should be penalized for including vague or generic statements and rewarded for providing detailed, precise information about the product and user experiences.\n",
    "\n",
    "<score>1</score> - All the opinions presented in the summary are generic.\n",
    "<score>2</score> - Most of the opinions presented are generic.\n",
    "<score>3</score> - Only around half of the opinions presented are specific.\n",
    "<score>4</score> - Most of the opinions presented in the summary are specific. Very few opinions are generic.\n",
    "<score>5</score> - All the opinions presented in the summary are specific \n",
    "\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all the opinions presented.\n",
    "2. Check if details are presented for the opinions. Classify each opinion as specific or generic.\n",
    "3. Count the number of generic and specific occurrences.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on specificity of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = specificity_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m3_sp_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d9bf69",
   "metadata": {},
   "source": [
    "# Mistral 2 summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55244e7",
   "metadata": {},
   "source": [
    "## Aspect Covergae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0304f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m2.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "    \n",
    "aspect_coverage_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating. You will then be given one summary written for the set of information. Your task is to rate the summary on one metric. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Aspect Coverage - Aspect Coverage measures how completely a summary captures the major features, characteristics, or attributes of a product that are prominently discussed in the original product information. Summaries should be penalized for missing any major aspects and rewarded for covering all important aspects thoroughly.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - Summary does not cover any important aspects present in the set of information.\n",
    "<score>2</score> - Summary does not cover most of the important aspects present in the set of information.\n",
    "<score>3</score> - Summary covers around half of the important aspects present in the set of information.\n",
    "<score>4</score> - Summary covers most of the important aspects present in the set of information.\n",
    "<score>5</score> - Summary covers all the important aspects discussed in the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the important aspects present in the set of information and list them with numbering.\n",
    "2. Identify the important aspects present in the summary and list them with numbering.\n",
    "3. Identify the important aspects covered by the summary that are present in the set of information and list them with numbering.\n",
    "4. Calculate the total number of important aspects covered by the summary that are present in the set of information.\n",
    "5. Calculate the total number of important aspects present in the set of information.\n",
    "6. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation of how much is the coverage and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = aspect_coverage_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m2_ac_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247cdae5",
   "metadata": {},
   "source": [
    "# Fluency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb84f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m2.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "fluency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Fluency : The quality of summary in terms of grammar, spelling, punctuation, capitalization, word choice, and sentence structure and should contain no errors. The summary should be easy to read, follow, comprehend and should contain no errors.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary is all garbled and does not make any sense.\n",
    "<score>2</score> - The summary has grammatical errors that make it hard to understand or sound unnatural.\n",
    "<score>3</score> - The summary has errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "<score>4</score> - The summary has very few errors, but it is easy to read, follow and comprehend.\n",
    "<score>5</score> - The summary is extremely fluent and is easy to read, follow and comprehend.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the sentences presented in the summary and list them with numbering.\n",
    "2. Go through each sentence and list down if there are any fluency problems.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on fluency of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = fluency_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m2_fl_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef9d89",
   "metadata": {},
   "source": [
    "# Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b2549",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m2.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "coherence_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Coherence - The collective quality of all sentences. The summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information.\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary lacks structure and logical flow, resulting in disjointed ideas and significant inconsistencies, making it confusing and challenging to follow.\n",
    "<score>2</score> - The summary attempts coherence but struggles with occasional lapses in logic, clarity issues, and insufficiently connected ideas, leading to a somewhat disjointed presentation.\n",
    "<score>3</score> - The summary displays a reasonable level of coherence with a logical sequence, yet occasional disruptions in flow and clarity, requiring some improvements for a smoother transition between ideas.\n",
    "<score>4</score> - The summary demonstrates strong coherence, maintaining a clear and organized flow with effective transitions and minimal inconsistencies, effectively conveying main points with clarity and precision.\n",
    "<score>5</score> - The summary showcases exceptional coherence with a flawless logical flow, impeccable transitions, and consistent clarity, presenting information in an impeccably organized and easily comprehensible manner.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary.\n",
    "2. Check if everything is presented in a clear and logical order. Give a clear step-by-step explanation of what you found and what is lacking.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation on coherence of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = coherence_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m2_co_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f23a4e",
   "metadata": {},
   "source": [
    "# Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432240c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m2.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "faithfulness_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Faithfulness - Faithfulness measures the extent to which every piece of information mentioned in the summary is verifiable, supported, present, or can be reasonably inferred from the input. The input includes product title, description, key features, specifications, reviews, and average rating. Summaries should be penalized if they contain information that cannot be verified from the provided input or if they make broad generalizations that are not supported by the input data.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "\n",
    "<score>1</score> - The summary is for a different product and is irrelevant/unrelated to the given set of information.\n",
    "<score>2</score> - The summary contains very few facts actually verifiable/supported/present/inferred from the set of information and contains a lot of hallucinated facts.\n",
    "<score>3</score> - The summary contains more than one piece of information that is not verifiable/present/inferred from the set of information.\n",
    "<score>4</score> - The summary contains only one piece of information that is not verifiable/supported/present/inferred from the the set of information.\n",
    "<score>5</score> - Every piece of information present in the summary is verifiable/supported/present/inferred from the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary that is not verifiable/supported/present/inferred from the set of information. Give a clear step-by-step explanation of what you found.\n",
    "2. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on faithfulness and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = faithfulness_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m2_fa_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d6413",
   "metadata": {},
   "source": [
    "# Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac5d3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m2.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "relevance_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Relevance - Relevance measures the selection of important information from the input, including product title, description, key features, specifications, reviews, and average rating. The summary should include only important and relevant information from the input. Summaries should not contain redundancies or excess information.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary misses all the important opinions majorly discussed in the set of information.\n",
    "<score>2</score> - The summary misses most of the important opinions majorly discussed in the set of information or mostly has redundant/excess/unimportant details\n",
    "<score>3</score> - The summary covers around half of the important opinions majorly discussed in the set of information. or contains redundant/excess/unimportant details.\n",
    "<score>4</score> - The summary covers most of the important opinions majorly discussed in the set of information and has very less amount of redundant/excess/unimportant details.\n",
    "<score>5</score> - The summary covers all the important opinions majorly discussed in the set of information and has no redundant/excess/unimportant details.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify all the important opinions majorly discussed in the set of information and list them with numbering.\n",
    "2. Identify the important opinions present in the summary and list them with numbering.\n",
    "3. Next identify how many important opinions are present in both summary and the set of information and list them with numbering\n",
    "4. Next idenify the how many redundant/excess/unimportant details does the summary have and list them with numbering.\n",
    "5. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on relevance and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = relevance_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m2_re_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563dc85",
   "metadata": {},
   "source": [
    "# Sentiment Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44408648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "\n",
    "with open(\"m2.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "sentiment_consistency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description: \n",
    "You will be given a set of information such as reviews, and average rating and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - None of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>2</score> - Very few of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>3</score> - Only around half of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>4</score> - Most of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>5</score> - All aspects present in summary have the same majority sentiment as in reviews.\n",
    "\n",
    "\n",
    "Product Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary to Evaluate: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the aspects and their sentiment present in the summary and list them with numbering.\n",
    "2. For the list of aspects identified, identify the majority sentiment from the reviews and list them with numbering.\n",
    "3. Next identify how many aspect and sentiment match between reviews and summary from above and list them with numbering.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on sentiment preservation of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "\n",
    "def evaluate_opinion_summary(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "\n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**Product {i + j + 1} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "\n",
    "with open(\"m2_sc_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e1981",
   "metadata": {},
   "source": [
    "# Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdf630",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m2.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "specificity_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Generic Opinion example: The battery is good.\n",
    "Specific Opinion example: The battery lasts for more than 12 hours on a single charge.\n",
    "\n",
    "Specificity - Specificity measures the level of detail and precision in the information and opinions presented in the summary. A specific summary provides concrete facts, measurements, or detailed descriptions about the product's features, performance, and user experiences. It avoids vague or general statements and instead offers precise information that gives readers a clear and thorough understanding of the product's characteristics and performance. \n",
    "\n",
    "Summaries should be penalized for including vague or generic statements and rewarded for providing detailed, precise information about the product and user experiences.\n",
    "\n",
    "<score>1</score> - All the opinions presented in the summary are generic.\n",
    "<score>2</score> - Most of the opinions presented are generic.\n",
    "<score>3</score> - Only around half of the opinions presented are specific.\n",
    "<score>4</score> - Most of the opinions presented in the summary are specific. Very few opinions are generic.\n",
    "<score>5</score> - All the opinions presented in the summary are specific \n",
    "\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all the opinions presented.\n",
    "2. Check if details are presented for the opinions. Classify each opinion as specific or generic.\n",
    "3. Count the number of generic and specific occurrences.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on specificity of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = specificity_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m2_sp_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1c2973",
   "metadata": {},
   "source": [
    "# Llama 3 summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15979f1",
   "metadata": {},
   "source": [
    "## Aspect Covergae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adcc245",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"l.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "    \n",
    "aspect_coverage_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating. You will then be given one summary written for the set of information. Your task is to rate the summary on one metric. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Aspect Coverage - Aspect Coverage measures how completely a summary captures the major features, characteristics, or attributes of a product that are prominently discussed in the original product information. Summaries should be penalized for missing any major aspects and rewarded for covering all important aspects thoroughly.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - Summary does not cover any important aspects present in the set of information.\n",
    "<score>2</score> - Summary does not cover most of the important aspects present in the set of information.\n",
    "<score>3</score> - Summary covers around half of the important aspects present in the set of information.\n",
    "<score>4</score> - Summary covers most of the important aspects present in the set of information.\n",
    "<score>5</score> - Summary covers all the important aspects discussed in the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the important aspects present in the set of information and list them with numbering.\n",
    "2. Identify the important aspects present in the summary and list them with numbering.\n",
    "3. Identify the important aspects covered by the summary that are present in the set of information and list them with numbering.\n",
    "4. Calculate the total number of important aspects covered by the summary that are present in the set of information.\n",
    "5. Calculate the total number of important aspects present in the set of information.\n",
    "6. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation of how much is the coverage and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = aspect_coverage_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"l_ac_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d5939f",
   "metadata": {},
   "source": [
    "# Fluency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c28ad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"l.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "fluency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Fluency : The quality of summary in terms of grammar, spelling, punctuation, capitalization, word choice, and sentence structure and should contain no errors. The summary should be easy to read, follow, comprehend and should contain no errors.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary is all garbled and does not make any sense.\n",
    "<score>2</score> - The summary has grammatical errors that make it hard to understand or sound unnatural.\n",
    "<score>3</score> - The summary has errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "<score>4</score> - The summary has very few errors, but it is easy to read, follow and comprehend.\n",
    "<score>5</score> - The summary is extremely fluent and is easy to read, follow and comprehend.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the sentences presented in the summary and list them with numbering.\n",
    "2. Go through each sentence and list down if there are any fluency problems.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on fluency of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = fluency_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"l_fl_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b4a98",
   "metadata": {},
   "source": [
    "# Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c98886",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"l.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "coherence_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Coherence - The collective quality of all sentences. The summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information.\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary lacks structure and logical flow, resulting in disjointed ideas and significant inconsistencies, making it confusing and challenging to follow.\n",
    "<score>2</score> - The summary attempts coherence but struggles with occasional lapses in logic, clarity issues, and insufficiently connected ideas, leading to a somewhat disjointed presentation.\n",
    "<score>3</score> - The summary displays a reasonable level of coherence with a logical sequence, yet occasional disruptions in flow and clarity, requiring some improvements for a smoother transition between ideas.\n",
    "<score>4</score> - The summary demonstrates strong coherence, maintaining a clear and organized flow with effective transitions and minimal inconsistencies, effectively conveying main points with clarity and precision.\n",
    "<score>5</score> - The summary showcases exceptional coherence with a flawless logical flow, impeccable transitions, and consistent clarity, presenting information in an impeccably organized and easily comprehensible manner.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary.\n",
    "2. Check if everything is presented in a clear and logical order. Give a clear step-by-step explanation of what you found and what is lacking.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation on coherence of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = coherence_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"l_co_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6a74a",
   "metadata": {},
   "source": [
    "# Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417bac8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"l.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "faithfulness_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Faithfulness - Faithfulness measures the extent to which every piece of information mentioned in the summary is verifiable, supported, present, or can be reasonably inferred from the input. The input includes product title, description, key features, specifications, reviews, and average rating. Summaries should be penalized if they contain information that cannot be verified from the provided input or if they make broad generalizations that are not supported by the input data.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "\n",
    "<score>1</score> - The summary is for a different product and is irrelevant/unrelated to the given set of information.\n",
    "<score>2</score> - The summary contains very few facts actually verifiable/supported/present/inferred from the set of information and contains a lot of hallucinated facts.\n",
    "<score>3</score> - The summary contains more than one piece of information that is not verifiable/present/inferred from the set of information.\n",
    "<score>4</score> - The summary contains only one piece of information that is not verifiable/supported/present/inferred from the the set of information.\n",
    "<score>5</score> - Every piece of information present in the summary is verifiable/supported/present/inferred from the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary that is not verifiable/supported/present/inferred from the set of information. Give a clear step-by-step explanation of what you found.\n",
    "2. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on faithfulness and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = faithfulness_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"l_fa_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca5bba",
   "metadata": {},
   "source": [
    "# Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d88ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"l.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "relevance_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Relevance - Relevance measures the selection of important information from the input, including product title, description, key features, specifications, reviews, and average rating. The summary should include only important and relevant information from the input. Summaries should not contain redundancies or excess information.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary misses all the important opinions majorly discussed in the set of information.\n",
    "<score>2</score> - The summary misses most of the important opinions majorly discussed in the set of information or mostly has redundant/excess/unimportant details\n",
    "<score>3</score> - The summary covers around half of the important opinions majorly discussed in the set of information. or contains redundant/excess/unimportant details.\n",
    "<score>4</score> - The summary covers most of the important opinions majorly discussed in the set of information and has very less amount of redundant/excess/unimportant details.\n",
    "<score>5</score> - The summary covers all the important opinions majorly discussed in the set of information and has no redundant/excess/unimportant details.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify all the important opinions majorly discussed in the set of information and list them with numbering.\n",
    "2. Identify the important opinions present in the summary and list them with numbering.\n",
    "3. Next identify how many important opinions are present in both summary and the set of information and list them with numbering\n",
    "4. Next idenify the how many redundant/excess/unimportant details does the summary have and list them with numbering.\n",
    "5. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on relevance and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = relevance_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"l_re_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f41ce7",
   "metadata": {},
   "source": [
    "# Sentiment Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842dec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "\n",
    "with open(\"l.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "sentiment_consistency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description: \n",
    "You will be given a set of information such as reviews, and average rating and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - None of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>2</score> - Very few of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>3</score> - Only around half of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>4</score> - Most of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>5</score> - All aspects present in summary have the same majority sentiment as in reviews.\n",
    "\n",
    "\n",
    "Product Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary to Evaluate: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the aspects and their sentiment present in the summary and list them with numbering.\n",
    "2. For the list of aspects identified, identify the majority sentiment from the reviews and list them with numbering.\n",
    "3. Next identify how many aspect and sentiment match between reviews and summary from above and list them with numbering.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on sentiment preservation of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "\n",
    "def evaluate_opinion_summary(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "\n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**Product {i + j + 1} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "\n",
    "with open(\"l_sc_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447c178",
   "metadata": {},
   "source": [
    "# Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b395a50",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"l.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "specificity_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Generic Opinion example: The battery is good.\n",
    "Specific Opinion example: The battery lasts for more than 12 hours on a single charge.\n",
    "\n",
    "Specificity - Specificity measures the level of detail and precision in the information and opinions presented in the summary. A specific summary provides concrete facts, measurements, or detailed descriptions about the product's features, performance, and user experiences. It avoids vague or general statements and instead offers precise information that gives readers a clear and thorough understanding of the product's characteristics and performance. \n",
    "\n",
    "Summaries should be penalized for including vague or generic statements and rewarded for providing detailed, precise information about the product and user experiences.\n",
    "\n",
    "<score>1</score> - All the opinions presented in the summary are generic.\n",
    "<score>2</score> - Most of the opinions presented are generic.\n",
    "<score>3</score> - Only around half of the opinions presented are specific.\n",
    "<score>4</score> - Most of the opinions presented in the summary are specific. Very few opinions are generic.\n",
    "<score>5</score> - All the opinions presented in the summary are specific \n",
    "\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all the opinions presented.\n",
    "2. Check if details are presented for the opinions. Classify each opinion as specific or generic.\n",
    "3. Count the number of generic and specific occurrences.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on specificity of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = specificity_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"l_sp_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb8423",
   "metadata": {},
   "source": [
    "# Gemma summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782bf07d",
   "metadata": {},
   "source": [
    "## Aspect Covergae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1856cc5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"g.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "    \n",
    "aspect_coverage_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating. You will then be given one summary written for the set of information. Your task is to rate the summary on one metric. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Aspect Coverage - Aspect Coverage measures how completely a summary captures the major features, characteristics, or attributes of a product that are prominently discussed in the original product information. Summaries should be penalized for missing any major aspects and rewarded for covering all important aspects thoroughly.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - Summary does not cover any important aspects present in the set of information.\n",
    "<score>2</score> - Summary does not cover most of the important aspects present in the set of information.\n",
    "<score>3</score> - Summary covers around half of the important aspects present in the set of information.\n",
    "<score>4</score> - Summary covers most of the important aspects present in the set of information.\n",
    "<score>5</score> - Summary covers all the important aspects discussed in the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the important aspects present in the set of information and list them with numbering.\n",
    "2. Identify the important aspects present in the summary and list them with numbering.\n",
    "3. Identify the important aspects covered by the summary that are present in the set of information and list them with numbering.\n",
    "4. Calculate the total number of important aspects covered by the summary that are present in the set of information.\n",
    "5. Calculate the total number of important aspects present in the set of information.\n",
    "6. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation of how much is the coverage and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = aspect_coverage_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"g_ac_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b142540",
   "metadata": {},
   "source": [
    "# Fluency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc0398",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"g.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "fluency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Fluency : The quality of summary in terms of grammar, spelling, punctuation, capitalization, word choice, and sentence structure and should contain no errors. The summary should be easy to read, follow, comprehend and should contain no errors.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary is all garbled and does not make any sense.\n",
    "<score>2</score> - The summary has grammatical errors that make it hard to understand or sound unnatural.\n",
    "<score>3</score> - The summary has errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "<score>4</score> - The summary has very few errors, but it is easy to read, follow and comprehend.\n",
    "<score>5</score> - The summary is extremely fluent and is easy to read, follow and comprehend.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the sentences presented in the summary and list them with numbering.\n",
    "2. Go through each sentence and list down if there are any fluency problems.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on fluency of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = fluency_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"g_fl_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb76f1",
   "metadata": {},
   "source": [
    "# Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec4cbe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"g.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "coherence_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Coherence - The collective quality of all sentences. The summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information.\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary lacks structure and logical flow, resulting in disjointed ideas and significant inconsistencies, making it confusing and challenging to follow.\n",
    "<score>2</score> - The summary attempts coherence but struggles with occasional lapses in logic, clarity issues, and insufficiently connected ideas, leading to a somewhat disjointed presentation.\n",
    "<score>3</score> - The summary displays a reasonable level of coherence with a logical sequence, yet occasional disruptions in flow and clarity, requiring some improvements for a smoother transition between ideas.\n",
    "<score>4</score> - The summary demonstrates strong coherence, maintaining a clear and organized flow with effective transitions and minimal inconsistencies, effectively conveying main points with clarity and precision.\n",
    "<score>5</score> - The summary showcases exceptional coherence with a flawless logical flow, impeccable transitions, and consistent clarity, presenting information in an impeccably organized and easily comprehensible manner.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary.\n",
    "2. Check if everything is presented in a clear and logical order. Give a clear step-by-step explanation of what you found and what is lacking.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation on coherence of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = coherence_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"g_co_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79061b16",
   "metadata": {},
   "source": [
    "# Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93152e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"g.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "faithfulness_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Faithfulness - Faithfulness measures the extent to which every piece of information mentioned in the summary is verifiable, supported, present, or can be reasonably inferred from the input. The input includes product title, description, key features, specifications, reviews, and average rating. Summaries should be penalized if they contain information that cannot be verified from the provided input or if they make broad generalizations that are not supported by the input data.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "\n",
    "<score>1</score> - The summary is for a different product and is irrelevant/unrelated to the given set of information.\n",
    "<score>2</score> - The summary contains very few facts actually verifiable/supported/present/inferred from the set of information and contains a lot of hallucinated facts.\n",
    "<score>3</score> - The summary contains more than one piece of information that is not verifiable/present/inferred from the set of information.\n",
    "<score>4</score> - The summary contains only one piece of information that is not verifiable/supported/present/inferred from the the set of information.\n",
    "<score>5</score> - Every piece of information present in the summary is verifiable/supported/present/inferred from the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary that is not verifiable/supported/present/inferred from the set of information. Give a clear step-by-step explanation of what you found.\n",
    "2. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on faithfulness and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = faithfulness_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"g_fa_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b7c6b",
   "metadata": {},
   "source": [
    "# Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06538ae6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"g.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "relevance_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Relevance - Relevance measures the selection of important information from the input, including product title, description, key features, specifications, reviews, and average rating. The summary should include only important and relevant information from the input. Summaries should not contain redundancies or excess information.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary misses all the important opinions majorly discussed in the set of information.\n",
    "<score>2</score> - The summary misses most of the important opinions majorly discussed in the set of information or mostly has redundant/excess/unimportant details\n",
    "<score>3</score> - The summary covers around half of the important opinions majorly discussed in the set of information. or contains redundant/excess/unimportant details.\n",
    "<score>4</score> - The summary covers most of the important opinions majorly discussed in the set of information and has very less amount of redundant/excess/unimportant details.\n",
    "<score>5</score> - The summary covers all the important opinions majorly discussed in the set of information and has no redundant/excess/unimportant details.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify all the important opinions majorly discussed in the set of information and list them with numbering.\n",
    "2. Identify the important opinions present in the summary and list them with numbering.\n",
    "3. Next identify how many important opinions are present in both summary and the set of information and list them with numbering\n",
    "4. Next idenify the how many redundant/excess/unimportant details does the summary have and list them with numbering.\n",
    "5. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on relevance and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = relevance_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"g_re_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84664693",
   "metadata": {},
   "source": [
    "# Sentiment Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "\n",
    "with open(\"g.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "sentiment_consistency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description: \n",
    "You will be given a set of information such as reviews, and average rating and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - None of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>2</score> - Very few of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>3</score> - Only around half of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>4</score> - Most of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>5</score> - All aspects present in summary have the same majority sentiment as in reviews.\n",
    "\n",
    "\n",
    "Product Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary to Evaluate: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the aspects and their sentiment present in the summary and list them with numbering.\n",
    "2. For the list of aspects identified, identify the majority sentiment from the reviews and list them with numbering.\n",
    "3. Next identify how many aspect and sentiment match between reviews and summary from above and list them with numbering.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on sentiment preservation of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "\n",
    "def evaluate_opinion_summary(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "\n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**Product {i + j + 1} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "\n",
    "with open(\"g_sc_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328fe5a",
   "metadata": {},
   "source": [
    "# Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d6c9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"g.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "specificity_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Generic Opinion example: The battery is good.\n",
    "Specific Opinion example: The battery lasts for more than 12 hours on a single charge.\n",
    "\n",
    "Specificity - Specificity measures the level of detail and precision in the information and opinions presented in the summary. A specific summary provides concrete facts, measurements, or detailed descriptions about the product's features, performance, and user experiences. It avoids vague or general statements and instead offers precise information that gives readers a clear and thorough understanding of the product's characteristics and performance. \n",
    "\n",
    "Summaries should be penalized for including vague or generic statements and rewarded for providing detailed, precise information about the product and user experiences.\n",
    "\n",
    "<score>1</score> - All the opinions presented in the summary are generic.\n",
    "<score>2</score> - Most of the opinions presented are generic.\n",
    "<score>3</score> - Only around half of the opinions presented are specific.\n",
    "<score>4</score> - Most of the opinions presented in the summary are specific. Very few opinions are generic.\n",
    "<score>5</score> - All the opinions presented in the summary are specific \n",
    "\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all the opinions presented.\n",
    "2. Check if details are presented for the opinions. Classify each opinion as specific or generic.\n",
    "3. Count the number of generic and specific occurrences.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on specificity of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = specificity_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"g_sp_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e39280",
   "metadata": {},
   "source": [
    "# viccuna summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66281946",
   "metadata": {},
   "source": [
    "## Aspect Covergae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fc8eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"v.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "    \n",
    "aspect_coverage_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating. You will then be given one summary written for the set of information. Your task is to rate the summary on one metric. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Aspect Coverage - Aspect Coverage measures how completely a summary captures the major features, characteristics, or attributes of a product that are prominently discussed in the original product information. Summaries should be penalized for missing any major aspects and rewarded for covering all important aspects thoroughly.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - Summary does not cover any important aspects present in the set of information.\n",
    "<score>2</score> - Summary does not cover most of the important aspects present in the set of information.\n",
    "<score>3</score> - Summary covers around half of the important aspects present in the set of information.\n",
    "<score>4</score> - Summary covers most of the important aspects present in the set of information.\n",
    "<score>5</score> - Summary covers all the important aspects discussed in the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the important aspects present in the set of information and list them with numbering.\n",
    "2. Identify the important aspects present in the summary and list them with numbering.\n",
    "3. Identify the important aspects covered by the summary that are present in the set of information and list them with numbering.\n",
    "4. Calculate the total number of important aspects covered by the summary that are present in the set of information.\n",
    "5. Calculate the total number of important aspects present in the set of information.\n",
    "6. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation of how much is the coverage and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = aspect_coverage_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"v_ac_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258a9d75",
   "metadata": {},
   "source": [
    "# Fluency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750668d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"v.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "fluency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Fluency : The quality of summary in terms of grammar, spelling, punctuation, capitalization, word choice, and sentence structure and should contain no errors. The summary should be easy to read, follow, comprehend and should contain no errors.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary is all garbled and does not make any sense.\n",
    "<score>2</score> - The summary has grammatical errors that make it hard to understand or sound unnatural.\n",
    "<score>3</score> - The summary has errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "<score>4</score> - The summary has very few errors, but it is easy to read, follow and comprehend.\n",
    "<score>5</score> - The summary is extremely fluent and is easy to read, follow and comprehend.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the sentences presented in the summary and list them with numbering.\n",
    "2. Go through each sentence and list down if there are any fluency problems.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on fluency of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = fluency_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"v_fl_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a033b9",
   "metadata": {},
   "source": [
    "# Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d8b15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"v.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "coherence_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Coherence - The collective quality of all sentences. The summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information.\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary lacks structure and logical flow, resulting in disjointed ideas and significant inconsistencies, making it confusing and challenging to follow.\n",
    "<score>2</score> - The summary attempts coherence but struggles with occasional lapses in logic, clarity issues, and insufficiently connected ideas, leading to a somewhat disjointed presentation.\n",
    "<score>3</score> - The summary displays a reasonable level of coherence with a logical sequence, yet occasional disruptions in flow and clarity, requiring some improvements for a smoother transition between ideas.\n",
    "<score>4</score> - The summary demonstrates strong coherence, maintaining a clear and organized flow with effective transitions and minimal inconsistencies, effectively conveying main points with clarity and precision.\n",
    "<score>5</score> - The summary showcases exceptional coherence with a flawless logical flow, impeccable transitions, and consistent clarity, presenting information in an impeccably organized and easily comprehensible manner.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary.\n",
    "2. Check if everything is presented in a clear and logical order. Give a clear step-by-step explanation of what you found and what is lacking.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation on coherence of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = coherence_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"v_co_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ea7de",
   "metadata": {},
   "source": [
    "# Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337187f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"v.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "faithfulness_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Faithfulness - Faithfulness measures the extent to which every piece of information mentioned in the summary is verifiable, supported, present, or can be reasonably inferred from the input. The input includes product title, description, key features, specifications, reviews, and average rating. Summaries should be penalized if they contain information that cannot be verified from the provided input or if they make broad generalizations that are not supported by the input data.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "\n",
    "<score>1</score> - The summary is for a different product and is irrelevant/unrelated to the given set of information.\n",
    "<score>2</score> - The summary contains very few facts actually verifiable/supported/present/inferred from the set of information and contains a lot of hallucinated facts.\n",
    "<score>3</score> - The summary contains more than one piece of information that is not verifiable/present/inferred from the set of information.\n",
    "<score>4</score> - The summary contains only one piece of information that is not verifiable/supported/present/inferred from the the set of information.\n",
    "<score>5</score> - Every piece of information present in the summary is verifiable/supported/present/inferred from the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary that is not verifiable/supported/present/inferred from the set of information. Give a clear step-by-step explanation of what you found.\n",
    "2. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on faithfulness and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = faithfulness_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"v_fa_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10811ad",
   "metadata": {},
   "source": [
    "# Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b77dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"v.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "relevance_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Relevance - Relevance measures the selection of important information from the input, including product title, description, key features, specifications, reviews, and average rating. The summary should include only important and relevant information from the input. Summaries should not contain redundancies or excess information.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary misses all the important opinions majorly discussed in the set of information.\n",
    "<score>2</score> - The summary misses most of the important opinions majorly discussed in the set of information or mostly has redundant/excess/unimportant details\n",
    "<score>3</score> - The summary covers around half of the important opinions majorly discussed in the set of information. or contains redundant/excess/unimportant details.\n",
    "<score>4</score> - The summary covers most of the important opinions majorly discussed in the set of information and has very less amount of redundant/excess/unimportant details.\n",
    "<score>5</score> - The summary covers all the important opinions majorly discussed in the set of information and has no redundant/excess/unimportant details.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify all the important opinions majorly discussed in the set of information and list them with numbering.\n",
    "2. Identify the important opinions present in the summary and list them with numbering.\n",
    "3. Next identify how many important opinions are present in both summary and the set of information and list them with numbering\n",
    "4. Next idenify the how many redundant/excess/unimportant details does the summary have and list them with numbering.\n",
    "5. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on relevance and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = relevance_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"v_re_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4a0d2",
   "metadata": {},
   "source": [
    "# Sentiment Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a245c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "\n",
    "with open(\"v.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "sentiment_consistency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description: \n",
    "You will be given a set of information such as reviews, and average rating and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - None of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>2</score> - Very few of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>3</score> - Only around half of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>4</score> - Most of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>5</score> - All aspects present in summary have the same majority sentiment as in reviews.\n",
    "\n",
    "\n",
    "Product Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary to Evaluate: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the aspects and their sentiment present in the summary and list them with numbering.\n",
    "2. For the list of aspects identified, identify the majority sentiment from the reviews and list them with numbering.\n",
    "3. Next identify how many aspect and sentiment match between reviews and summary from above and list them with numbering.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on sentiment preservation of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "\n",
    "def evaluate_opinion_summary(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "\n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**Product {i + j + 1} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "\n",
    "with open(\"v_sc_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e736677",
   "metadata": {},
   "source": [
    "# Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeded2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"v.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "specificity_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Generic Opinion example: The battery is good.\n",
    "Specific Opinion example: The battery lasts for more than 12 hours on a single charge.\n",
    "\n",
    "Specificity - Specificity measures the level of detail and precision in the information and opinions presented in the summary. A specific summary provides concrete facts, measurements, or detailed descriptions about the product's features, performance, and user experiences. It avoids vague or general statements and instead offers precise information that gives readers a clear and thorough understanding of the product's characteristics and performance. \n",
    "\n",
    "Summaries should be penalized for including vague or generic statements and rewarded for providing detailed, precise information about the product and user experiences.\n",
    "\n",
    "<score>1</score> - All the opinions presented in the summary are generic.\n",
    "<score>2</score> - Most of the opinions presented are generic.\n",
    "<score>3</score> - Only around half of the opinions presented are specific.\n",
    "<score>4</score> - Most of the opinions presented in the summary are specific. Very few opinions are generic.\n",
    "<score>5</score> - All the opinions presented in the summary are specific \n",
    "\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all the opinions presented.\n",
    "2. Check if details are presented for the opinions. Classify each opinion as specific or generic.\n",
    "3. Count the number of generic and specific occurrences.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on specificity of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = specificity_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"v_sp_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3d681",
   "metadata": {},
   "source": [
    "# Zephyr summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c46dcde",
   "metadata": {},
   "source": [
    "## Aspect Covergae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7894e77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"z.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "    \n",
    "aspect_coverage_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating. You will then be given one summary written for the set of information. Your task is to rate the summary on one metric. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Aspect Coverage - Aspect Coverage measures how completely a summary captures the major features, characteristics, or attributes of a product that are prominently discussed in the original product information. Summaries should be penalized for missing any major aspects and rewarded for covering all important aspects thoroughly.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - Summary does not cover any important aspects present in the set of information.\n",
    "<score>2</score> - Summary does not cover most of the important aspects present in the set of information.\n",
    "<score>3</score> - Summary covers around half of the important aspects present in the set of information.\n",
    "<score>4</score> - Summary covers most of the important aspects present in the set of information.\n",
    "<score>5</score> - Summary covers all the important aspects discussed in the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the important aspects present in the set of information and list them with numbering.\n",
    "2. Identify the important aspects present in the summary and list them with numbering.\n",
    "3. Identify the important aspects covered by the summary that are present in the set of information and list them with numbering.\n",
    "4. Calculate the total number of important aspects covered by the summary that are present in the set of information.\n",
    "5. Calculate the total number of important aspects present in the set of information.\n",
    "6. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation of how much is the coverage and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = aspect_coverage_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"z_ac_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b692e1",
   "metadata": {},
   "source": [
    "# Fluency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf09e5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"z.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "fluency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Fluency : The quality of summary in terms of grammar, spelling, punctuation, capitalization, word choice, and sentence structure and should contain no errors. The summary should be easy to read, follow, comprehend and should contain no errors.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary is all garbled and does not make any sense.\n",
    "<score>2</score> - The summary has grammatical errors that make it hard to understand or sound unnatural.\n",
    "<score>3</score> - The summary has errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "<score>4</score> - The summary has very few errors, but it is easy to read, follow and comprehend.\n",
    "<score>5</score> - The summary is extremely fluent and is easy to read, follow and comprehend.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the sentences presented in the summary and list them with numbering.\n",
    "2. Go through each sentence and list down if there are any fluency problems.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on fluency of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = fluency_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"z_fl_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b292dbf",
   "metadata": {},
   "source": [
    "# Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb8a029",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"z.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "coherence_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Coherence - The collective quality of all sentences. The summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information.\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary lacks structure and logical flow, resulting in disjointed ideas and significant inconsistencies, making it confusing and challenging to follow.\n",
    "<score>2</score> - The summary attempts coherence but struggles with occasional lapses in logic, clarity issues, and insufficiently connected ideas, leading to a somewhat disjointed presentation.\n",
    "<score>3</score> - The summary displays a reasonable level of coherence with a logical sequence, yet occasional disruptions in flow and clarity, requiring some improvements for a smoother transition between ideas.\n",
    "<score>4</score> - The summary demonstrates strong coherence, maintaining a clear and organized flow with effective transitions and minimal inconsistencies, effectively conveying main points with clarity and precision.\n",
    "<score>5</score> - The summary showcases exceptional coherence with a flawless logical flow, impeccable transitions, and consistent clarity, presenting information in an impeccably organized and easily comprehensible manner.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary.\n",
    "2. Check if everything is presented in a clear and logical order. Give a clear step-by-step explanation of what you found and what is lacking.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation on coherence of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = coherence_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"z_co_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c2681",
   "metadata": {},
   "source": [
    "# Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f5517",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"z.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "faithfulness_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Faithfulness - Faithfulness measures the extent to which every piece of information mentioned in the summary is verifiable, supported, present, or can be reasonably inferred from the input. The input includes product title, description, key features, specifications, reviews, and average rating. Summaries should be penalized if they contain information that cannot be verified from the provided input or if they make broad generalizations that are not supported by the input data.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "\n",
    "<score>1</score> - The summary is for a different product and is irrelevant/unrelated to the given set of information.\n",
    "<score>2</score> - The summary contains very few facts actually verifiable/supported/present/inferred from the set of information and contains a lot of hallucinated facts.\n",
    "<score>3</score> - The summary contains more than one piece of information that is not verifiable/present/inferred from the set of information.\n",
    "<score>4</score> - The summary contains only one piece of information that is not verifiable/supported/present/inferred from the the set of information.\n",
    "<score>5</score> - Every piece of information present in the summary is verifiable/supported/present/inferred from the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary that is not verifiable/supported/present/inferred from the set of information. Give a clear step-by-step explanation of what you found.\n",
    "2. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on faithfulness and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = faithfulness_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"z_fa_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae0ea2",
   "metadata": {},
   "source": [
    "# Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcec753",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"z.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "relevance_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Relevance - Relevance measures the selection of important information from the input, including product title, description, key features, specifications, reviews, and average rating. The summary should include only important and relevant information from the input. Summaries should not contain redundancies or excess information.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary misses all the important opinions majorly discussed in the set of information.\n",
    "<score>2</score> - The summary misses most of the important opinions majorly discussed in the set of information or mostly has redundant/excess/unimportant details\n",
    "<score>3</score> - The summary covers around half of the important opinions majorly discussed in the set of information. or contains redundant/excess/unimportant details.\n",
    "<score>4</score> - The summary covers most of the important opinions majorly discussed in the set of information and has very less amount of redundant/excess/unimportant details.\n",
    "<score>5</score> - The summary covers all the important opinions majorly discussed in the set of information and has no redundant/excess/unimportant details.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify all the important opinions majorly discussed in the set of information and list them with numbering.\n",
    "2. Identify the important opinions present in the summary and list them with numbering.\n",
    "3. Next identify how many important opinions are present in both summary and the set of information and list them with numbering\n",
    "4. Next idenify the how many redundant/excess/unimportant details does the summary have and list them with numbering.\n",
    "5. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on relevance and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = relevance_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"z_re_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db35467",
   "metadata": {},
   "source": [
    "# Sentiment Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6220e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "\n",
    "with open(\"z.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "sentiment_consistency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description: \n",
    "You will be given a set of information such as reviews, and average rating and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - None of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>2</score> - Very few of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>3</score> - Only around half of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>4</score> - Most of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>5</score> - All aspects present in summary have the same majority sentiment as in reviews.\n",
    "\n",
    "\n",
    "Product Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary to Evaluate: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the aspects and their sentiment present in the summary and list them with numbering.\n",
    "2. For the list of aspects identified, identify the majority sentiment from the reviews and list them with numbering.\n",
    "3. Next identify how many aspect and sentiment match between reviews and summary from above and list them with numbering.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on sentiment preservation of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "\n",
    "def evaluate_opinion_summary(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "\n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**Product {i + j + 1} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "\n",
    "with open(\"z_sc_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f38427",
   "metadata": {},
   "source": [
    "# Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0ff20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"z.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "specificity_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Generic Opinion example: The battery is good.\n",
    "Specific Opinion example: The battery lasts for more than 12 hours on a single charge.\n",
    "\n",
    "Specificity - Specificity measures the level of detail and precision in the information and opinions presented in the summary. A specific summary provides concrete facts, measurements, or detailed descriptions about the product's features, performance, and user experiences. It avoids vague or general statements and instead offers precise information that gives readers a clear and thorough understanding of the product's characteristics and performance. \n",
    "\n",
    "Summaries should be penalized for including vague or generic statements and rewarded for providing detailed, precise information about the product and user experiences.\n",
    "\n",
    "<score>1</score> - All the opinions presented in the summary are generic.\n",
    "<score>2</score> - Most of the opinions presented are generic.\n",
    "<score>3</score> - Only around half of the opinions presented are specific.\n",
    "<score>4</score> - Most of the opinions presented in the summary are specific. Very few opinions are generic.\n",
    "<score>5</score> - All the opinions presented in the summary are specific \n",
    "\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all the opinions presented.\n",
    "2. Check if details are presented for the opinions. Classify each opinion as specific or generic.\n",
    "3. Count the number of generic and specific occurrences.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on specificity of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = specificity_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"z_sp_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4fdaa",
   "metadata": {},
   "source": [
    "# GPT 4o summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e5209",
   "metadata": {},
   "source": [
    "## Aspect Covergae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511a21a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"4o.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "    \n",
    "aspect_coverage_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating. You will then be given one summary written for the set of information. Your task is to rate the summary on one metric. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Aspect Coverage - Aspect Coverage measures how completely a summary captures the major features, characteristics, or attributes of a product that are prominently discussed in the original product information. Summaries should be penalized for missing any major aspects and rewarded for covering all important aspects thoroughly.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - Summary does not cover any important aspects present in the set of information.\n",
    "<score>2</score> - Summary does not cover most of the important aspects present in the set of information.\n",
    "<score>3</score> - Summary covers around half of the important aspects present in the set of information.\n",
    "<score>4</score> - Summary covers most of the important aspects present in the set of information.\n",
    "<score>5</score> - Summary covers all the important aspects discussed in the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the important aspects present in the set of information and list them with numbering.\n",
    "2. Identify the important aspects present in the summary and list them with numbering.\n",
    "3. Identify the important aspects covered by the summary that are present in the set of information and list them with numbering.\n",
    "4. Calculate the total number of important aspects covered by the summary that are present in the set of information.\n",
    "5. Calculate the total number of important aspects present in the set of information.\n",
    "6. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation of how much is the coverage and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = aspect_coverage_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"4o_ac_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3e610",
   "metadata": {},
   "source": [
    "# Fluency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49559b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"4o.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "fluency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Fluency : The quality of summary in terms of grammar, spelling, punctuation, capitalization, word choice, and sentence structure and should contain no errors. The summary should be easy to read, follow, comprehend and should contain no errors.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary is all garbled and does not make any sense.\n",
    "<score>2</score> - The summary has grammatical errors that make it hard to understand or sound unnatural.\n",
    "<score>3</score> - The summary has errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "<score>4</score> - The summary has very few errors, but it is easy to read, follow and comprehend.\n",
    "<score>5</score> - The summary is extremely fluent and is easy to read, follow and comprehend.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the sentences presented in the summary and list them with numbering.\n",
    "2. Go through each sentence and list down if there are any fluency problems.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on fluency of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = fluency_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"4o_fl_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f338b78e",
   "metadata": {},
   "source": [
    "# Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d323f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"4o.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "coherence_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Coherence - The collective quality of all sentences. The summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information.\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary lacks structure and logical flow, resulting in disjointed ideas and significant inconsistencies, making it confusing and challenging to follow.\n",
    "<score>2</score> - The summary attempts coherence but struggles with occasional lapses in logic, clarity issues, and insufficiently connected ideas, leading to a somewhat disjointed presentation.\n",
    "<score>3</score> - The summary displays a reasonable level of coherence with a logical sequence, yet occasional disruptions in flow and clarity, requiring some improvements for a smoother transition between ideas.\n",
    "<score>4</score> - The summary demonstrates strong coherence, maintaining a clear and organized flow with effective transitions and minimal inconsistencies, effectively conveying main points with clarity and precision.\n",
    "<score>5</score> - The summary showcases exceptional coherence with a flawless logical flow, impeccable transitions, and consistent clarity, presenting information in an impeccably organized and easily comprehensible manner.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary.\n",
    "2. Check if everything is presented in a clear and logical order. Give a clear step-by-step explanation of what you found and what is lacking.\n",
    "3. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation on coherence of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = coherence_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"4o_co_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ec092",
   "metadata": {},
   "source": [
    "# Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032fbd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"4o.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "faithfulness_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Faithfulness - Faithfulness measures the extent to which every piece of information mentioned in the summary is verifiable, supported, present, or can be reasonably inferred from the input. The input includes product title, description, key features, specifications, reviews, and average rating. Summaries should be penalized if they contain information that cannot be verified from the provided input or if they make broad generalizations that are not supported by the input data.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "\n",
    "<score>1</score> - The summary is for a different product and is irrelevant/unrelated to the given set of information.\n",
    "<score>2</score> - The summary contains very few facts actually verifiable/supported/present/inferred from the set of information and contains a lot of hallucinated facts.\n",
    "<score>3</score> - The summary contains more than one piece of information that is not verifiable/present/inferred from the set of information.\n",
    "<score>4</score> - The summary contains only one piece of information that is not verifiable/supported/present/inferred from the the set of information.\n",
    "<score>5</score> - Every piece of information present in the summary is verifiable/supported/present/inferred from the set of information.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all pieces of information in the summary that is not verifiable/supported/present/inferred from the set of information. Give a clear step-by-step explanation of what you found.\n",
    "2. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on faithfulness and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = faithfulness_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"4o_fa_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c64f37",
   "metadata": {},
   "source": [
    "# Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c88f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"4o.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "relevance_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Relevance - Relevance measures the selection of important information from the input, including product title, description, key features, specifications, reviews, and average rating. The summary should include only important and relevant information from the input. Summaries should not contain redundancies or excess information.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - The summary misses all the important opinions majorly discussed in the set of information.\n",
    "<score>2</score> - The summary misses most of the important opinions majorly discussed in the set of information or mostly has redundant/excess/unimportant details\n",
    "<score>3</score> - The summary covers around half of the important opinions majorly discussed in the set of information. or contains redundant/excess/unimportant details.\n",
    "<score>4</score> - The summary covers most of the important opinions majorly discussed in the set of information and has very less amount of redundant/excess/unimportant details.\n",
    "<score>5</score> - The summary covers all the important opinions majorly discussed in the set of information and has no redundant/excess/unimportant details.\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify all the important opinions majorly discussed in the set of information and list them with numbering.\n",
    "2. Identify the important opinions present in the summary and list them with numbering.\n",
    "3. Next identify how many important opinions are present in both summary and the set of information and list them with numbering\n",
    "4. Next idenify the how many redundant/excess/unimportant details does the summary have and list them with numbering.\n",
    "5. Finally use the evaluation criteria to output only a single score within <score></score> tags.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on relevance and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = relevance_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"4o_re_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a721f79d",
   "metadata": {},
   "source": [
    "# Sentiment Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "\n",
    "with open(\"4o.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "sentiment_consistency_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description: \n",
    "You will be given a set of information such as reviews, and average rating and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "Following are the scores and the evaluation criteria according to which scores must be assigned.\n",
    "<score>1</score> - None of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>2</score> - Very few of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>3</score> - Only around half of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>4</score> - Most of the aspects present in summary have the same majority sentiment as in reviews.\n",
    "<score>5</score> - All aspects present in summary have the same majority sentiment as in reviews.\n",
    "\n",
    "\n",
    "Product Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary to Evaluate: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Identify the aspects and their sentiment present in the summary and list them with numbering.\n",
    "2. For the list of aspects identified, identify the majority sentiment from the reviews and list them with numbering.\n",
    "3. Next identify how many aspect and sentiment match between reviews and summary from above and list them with numbering.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on sentiment preservation of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "\n",
    "def evaluate_opinion_summary(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "\n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**Product {i + j + 1} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "\n",
    "with open(\"4o_sc_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd9e87",
   "metadata": {},
   "source": [
    "# Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"4o.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "specificity_prompt_template = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Task Description:\n",
    "You will be given a set of information such as product title, description, key features, specifications, reviews, and average rating.  and a corresponding summary. Make sure you understand the following evaluation metric very clearly. Your task is to rate the summary corresponding to the given set of information on the evaluation criteria.\n",
    "\n",
    "Evaluation Criteria:\n",
    "Generic Opinion example: The battery is good.\n",
    "Specific Opinion example: The battery lasts for more than 12 hours on a single charge.\n",
    "\n",
    "Specificity - Specificity measures the level of detail and precision in the information and opinions presented in the summary. A specific summary provides concrete facts, measurements, or detailed descriptions about the product's features, performance, and user experiences. It avoids vague or general statements and instead offers precise information that gives readers a clear and thorough understanding of the product's characteristics and performance. \n",
    "\n",
    "Summaries should be penalized for including vague or generic statements and rewarded for providing detailed, precise information about the product and user experiences.\n",
    "\n",
    "<score>1</score> - All the opinions presented in the summary are generic.\n",
    "<score>2</score> - Most of the opinions presented are generic.\n",
    "<score>3</score> - Only around half of the opinions presented are specific.\n",
    "<score>4</score> - Most of the opinions presented in the summary are specific. Very few opinions are generic.\n",
    "<score>5</score> - All the opinions presented in the summary are specific \n",
    "\n",
    "\n",
    "Product Title: {product_title}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Key Features: {key_features}\n",
    "\n",
    "Specifications: {specifications}\n",
    "\n",
    "Reviews: {reviews}\n",
    "\n",
    "Average Rating: {average_rating}\n",
    "\n",
    "Summary: {Product_Opinion_Summary}\n",
    "\n",
    "Instructions:\n",
    "Let's go step-by-step. Follow the following steps strictly while giving the response:\n",
    "\n",
    "1. Go through the summary and list down all the opinions presented.\n",
    "2. Check if details are presented for the opinions. Classify each opinion as specific or generic.\n",
    "3. Count the number of generic and specific occurrences.\n",
    "4. Finally use the previous information to output only a single score within <score></score> tags only using the evaluation criteria.\n",
    "\n",
    "Note: Strictly give the score within <score></score> tags only e.g Score- <score>5</score>.\n",
    "\n",
    "First give a detailed explanation only on specificity of the summary and then finally give a single score following the format: Score- <score>5</score> <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Function to format specifications\n",
    "def format_specifications(specifications):\n",
    "    formatted_specs = []\n",
    "    for spec in specifications:\n",
    "        spec_str = f\"{spec['key']}:\\n\"\n",
    "        for value in spec['values']:\n",
    "            spec_str += f\"- {value['key']}: {value['value']}\\n\"\n",
    "        formatted_specs.append(spec_str)\n",
    "    return \"\\n\".join(formatted_specs)\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_opinion_summary(product):\n",
    "    product_title = product.get(\"product_title\", \"\")\n",
    "    description = product.get(\"description\", \"\") if product.get(\"description\") else \"N/A\"\n",
    "    key_features = \"\\n\".join(product.get(\"key_features\", [])) if product.get(\"key_features\") else \"N/A\"\n",
    "    specifications = format_specifications(product.get(\"specifications\", [])) if product.get(\"specifications\") else \"N/A\"\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "\n",
    "    prompt = specificity_prompt_template.format(\n",
    "        product_title=product_title,\n",
    "        description=description,\n",
    "        key_features=key_features,\n",
    "        specifications=specifications,\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_opinion_summary(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"4o_sp_tj_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db60d661-8545-44d0-9f6a-eb9fb5b25c72",
   "metadata": {},
   "source": [
    "# Ours_Dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968ac01-d26b-4901-9180-461ca7c3c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m3_50.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "# System message and updated prompt template for sentiment consistency\n",
    "system_msg = \"\"\"You are a highly skilled expert in evaluating the sentiment consistency of product opinion summaries. Your expertise lies in analyzing summaries created from product reviews and ratings.\n",
    "\n",
    "Your primary responsibilities are:\n",
    "\n",
    "1. Carefully examine the provided product reviews and summary.\n",
    "2. Meticulously follow all instructions in the prompt faithfully and truthfully.\n",
    "3. Evaluate the summary's sentiment consistency with utmost accuracy and impartiality.\n",
    "4. Assign a single score (1-5) based on the quality of sentiment consistency, adhering strictly to the given evaluation criteria.\n",
    "5. Follow the specified format for all responses.\n",
    "\n",
    "Your expert evaluation is crucial for maintaining the accuracy of sentiment representation in product summaries. Approach each evaluation with diligence and attention to detail.\n",
    "\"\"\"\n",
    "\n",
    "sentiment_consistency_prompt_template = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Task Description: \n",
    "\n",
    "You will be given product reviews and an average rating. Next, you will be provided with one summary created using this product information. Your task is to carefully follow each evaluation criterion and instruction and always provide a faithful, truthful, and accurate output in the specified format. You must evaluate and assign a single score ranging from 1 to 5, to the summary, according to the metric called sentiment consistency.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "<score>1</score> - Very Poor (Completely Inconsistent, 0-20% consistent)\n",
    "\n",
    "- None of the aspects mentioned in the summary reflect the majority sentiment from the reviews.\n",
    "- The summary completely misrepresents the overall sentiment of the product.\n",
    "- Readers would get an entirely inaccurate impression of users' opinions.\n",
    "\n",
    "<score>2</score> - Poor (Mostly Inconsistent, 21-50% consistent)\n",
    "\n",
    "- Very few aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely misrepresents the sentiments expressed in the reviews.\n",
    "- Readers would get a mostly inaccurate impression of users' opinions.\n",
    "\n",
    "<score>3</score> - OK (Partially Consistent, 51-70% consistent)\n",
    "\n",
    "- About half to two-thirds of the aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary partially represents the sentiments expressed in the reviews.\n",
    "- Readers would get a mixed impression of users' opinions, with some accuracy.\n",
    "\n",
    "<score>4</score> - Good (Mostly Consistent, 71-90% consistent)\n",
    "\n",
    "- Most aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely represents the sentiments expressed in the reviews, with minor inconsistencies.\n",
    "- Readers would get a mostly accurate impression of users' opinions.\n",
    "\n",
    "<score>5</score> - Excellent (Completely Consistent, 91-100% consistent)\n",
    "\n",
    "- All or nearly all aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary perfectly represents the sentiments expressed in the reviews.\n",
    "- Readers would get a fully accurate impression of users' opinions.\n",
    "\n",
    "\n",
    "Product Reviews: {{reviews}}\n",
    "\n",
    "Average Rating: {{average_rating}}\n",
    "\n",
    "Summary to Evaluate: {{Product_Opinion_Summary}}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Follow these steps strictly:\n",
    "\n",
    "Step 1. Identify all aspects of the product mentioned in the reviews and determine the majority sentiment for each aspect. List these with numbering.\n",
    "Step 2. Identify all aspects and their associated sentiments mentioned in the summary. List these with numbering.\n",
    "Step 3. Compare the aspects and sentiments from steps 1 and 2. Identify which aspects in the summary accurately reflect the majority sentiment from the reviews. List these with numbering.\n",
    "Step 4. Identify any aspects in the summary that misrepresent or fail to accurately convey the majority sentiment from the reviews. List these with numbering.\n",
    "Step 5. Calculate the percentage of aspects in the summary that accurately reflect the majority sentiment from the reviews.\n",
    "Step 6. Carefully match the observed sentiment consistency level to the descriptions in the evaluation criteria:\n",
    "\n",
    "   - Completely Inconsistent (0-20% consistent): Score 1 (Very Poor)\n",
    "   - Mostly Inconsistent (21-50% consistent): Score 2 (Poor)\n",
    "   - Partially Consistent (51-70% consistent): Score 3 (OK)\n",
    "   - Mostly Consistent (71-90% consistent): Score 4 (Good)\n",
    "   - Completely Consistent (91-100% consistent): Score 5 (Excellent)\n",
    "\n",
    "Step 7. Assign a score based on the sentiment consistency level, using the exact format shown in the evaluation criteria.\n",
    "\n",
    "Your response should follow this structure:\n",
    "\n",
    "1. Provide a detailed explanation of the sentiment consistency level of the summary, including specific examples of accurately and inaccurately represented sentiments for different aspects.\n",
    "2. Clearly state which sentiment consistency level this falls into, referencing the evaluation criteria.\n",
    "3. Assign a single score based on the sentiment consistency level, using the exact format shown below.\n",
    "\n",
    "Score format: Score- <score>X</score>\n",
    "Where X is the assigned score (1, 2, 3, 4, or 5) based on the sentiment consistency levels in the evaluation criteria.\n",
    "\n",
    "Remember, your final score must always be presented in this exact format, with no deviations. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_sentiment_consistency(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "    \n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_sentiment_consistency(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m3_sc_our_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639b15a-c490-48a8-a9bc-41f4518ece8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"m2.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "# System message and updated prompt template for sentiment consistency\n",
    "system_msg = \"\"\"You are a highly skilled expert in evaluating the sentiment consistency of product opinion summaries. Your expertise lies in analyzing summaries created from product reviews and ratings.\n",
    "\n",
    "Your primary responsibilities are:\n",
    "\n",
    "1. Carefully examine the provided product reviews and summary.\n",
    "2. Meticulously follow all instructions in the prompt faithfully and truthfully.\n",
    "3. Evaluate the summary's sentiment consistency with utmost accuracy and impartiality.\n",
    "4. Assign a single score (1-5) based on the quality of sentiment consistency, adhering strictly to the given evaluation criteria.\n",
    "5. Follow the specified format for all responses.\n",
    "\n",
    "Your expert evaluation is crucial for maintaining the accuracy of sentiment representation in product summaries. Approach each evaluation with diligence and attention to detail.\n",
    "\"\"\"\n",
    "\n",
    "sentiment_consistency_prompt_template = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Task Description: \n",
    "\n",
    "You will be given product reviews and an average rating. Next, you will be provided with one summary created using this product information. Your task is to carefully follow each evaluation criterion and instruction and always provide a faithful, truthful, and accurate output in the specified format. You must evaluate and assign a single score ranging from 1 to 5, to the summary, according to the metric called sentiment consistency.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "<score>1</score> - Very Poor (Completely Inconsistent, 0-20% consistent)\n",
    "\n",
    "- None of the aspects mentioned in the summary reflect the majority sentiment from the reviews.\n",
    "- The summary completely misrepresents the overall sentiment of the product.\n",
    "- Readers would get an entirely inaccurate impression of users' opinions.\n",
    "\n",
    "<score>2</score> - Poor (Mostly Inconsistent, 21-50% consistent)\n",
    "\n",
    "- Very few aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely misrepresents the sentiments expressed in the reviews.\n",
    "- Readers would get a mostly inaccurate impression of users' opinions.\n",
    "\n",
    "<score>3</score> - OK (Partially Consistent, 51-70% consistent)\n",
    "\n",
    "- About half to two-thirds of the aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary partially represents the sentiments expressed in the reviews.\n",
    "- Readers would get a mixed impression of users' opinions, with some accuracy.\n",
    "\n",
    "<score>4</score> - Good (Mostly Consistent, 71-90% consistent)\n",
    "\n",
    "- Most aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely represents the sentiments expressed in the reviews, with minor inconsistencies.\n",
    "- Readers would get a mostly accurate impression of users' opinions.\n",
    "\n",
    "<score>5</score> - Excellent (Completely Consistent, 91-100% consistent)\n",
    "\n",
    "- All or nearly all aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary perfectly represents the sentiments expressed in the reviews.\n",
    "- Readers would get a fully accurate impression of users' opinions.\n",
    "\n",
    "\n",
    "Product Reviews: {{reviews}}\n",
    "\n",
    "Average Rating: {{average_rating}}\n",
    "\n",
    "Summary to Evaluate: {{Product_Opinion_Summary}}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Follow these steps strictly:\n",
    "\n",
    "Step 1. Identify all aspects of the product mentioned in the reviews and determine the majority sentiment for each aspect. List these with numbering.\n",
    "Step 2. Identify all aspects and their associated sentiments mentioned in the summary. List these with numbering.\n",
    "Step 3. Compare the aspects and sentiments from steps 1 and 2. Identify which aspects in the summary accurately reflect the majority sentiment from the reviews. List these with numbering.\n",
    "Step 4. Identify any aspects in the summary that misrepresent or fail to accurately convey the majority sentiment from the reviews. List these with numbering.\n",
    "Step 5. Calculate the percentage of aspects in the summary that accurately reflect the majority sentiment from the reviews.\n",
    "Step 6. Carefully match the observed sentiment consistency level to the descriptions in the evaluation criteria:\n",
    "\n",
    "   - Completely Inconsistent (0-20% consistent): Score 1 (Very Poor)\n",
    "   - Mostly Inconsistent (21-50% consistent): Score 2 (Poor)\n",
    "   - Partially Consistent (51-70% consistent): Score 3 (OK)\n",
    "   - Mostly Consistent (71-90% consistent): Score 4 (Good)\n",
    "   - Completely Consistent (91-100% consistent): Score 5 (Excellent)\n",
    "\n",
    "Step 7. Assign a score based on the sentiment consistency level, using the exact format shown in the evaluation criteria.\n",
    "\n",
    "Your response should follow this structure:\n",
    "\n",
    "1. Provide a detailed explanation of the sentiment consistency level of the summary, including specific examples of accurately and inaccurately represented sentiments for different aspects.\n",
    "2. Clearly state which sentiment consistency level this falls into, referencing the evaluation criteria.\n",
    "3. Assign a single score based on the sentiment consistency level, using the exact format shown below.\n",
    "\n",
    "Score format: Score- <score>X</score>\n",
    "Where X is the assigned score (1, 2, 3, 4, or 5) based on the sentiment consistency levels in the evaluation criteria.\n",
    "\n",
    "Remember, your final score must always be presented in this exact format, with no deviations. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_sentiment_consistency(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "    \n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_sentiment_consistency(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"m2_sc_our_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826e0a5-2701-4ab0-b9b2-ca26aee67210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"l.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "# System message and updated prompt template for sentiment consistency\n",
    "system_msg = \"\"\"You are a highly skilled expert in evaluating the sentiment consistency of product opinion summaries. Your expertise lies in analyzing summaries created from product reviews and ratings.\n",
    "\n",
    "Your primary responsibilities are:\n",
    "\n",
    "1. Carefully examine the provided product reviews and summary.\n",
    "2. Meticulously follow all instructions in the prompt faithfully and truthfully.\n",
    "3. Evaluate the summary's sentiment consistency with utmost accuracy and impartiality.\n",
    "4. Assign a single score (1-5) based on the quality of sentiment consistency, adhering strictly to the given evaluation criteria.\n",
    "5. Follow the specified format for all responses.\n",
    "\n",
    "Your expert evaluation is crucial for maintaining the accuracy of sentiment representation in product summaries. Approach each evaluation with diligence and attention to detail.\n",
    "\"\"\"\n",
    "\n",
    "sentiment_consistency_prompt_template = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Task Description: \n",
    "\n",
    "You will be given product reviews and an average rating. Next, you will be provided with one summary created using this product information. Your task is to carefully follow each evaluation criterion and instruction and always provide a faithful, truthful, and accurate output in the specified format. You must evaluate and assign a single score ranging from 1 to 5, to the summary, according to the metric called sentiment consistency.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "<score>1</score> - Very Poor (Completely Inconsistent, 0-20% consistent)\n",
    "\n",
    "- None of the aspects mentioned in the summary reflect the majority sentiment from the reviews.\n",
    "- The summary completely misrepresents the overall sentiment of the product.\n",
    "- Readers would get an entirely inaccurate impression of users' opinions.\n",
    "\n",
    "<score>2</score> - Poor (Mostly Inconsistent, 21-50% consistent)\n",
    "\n",
    "- Very few aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely misrepresents the sentiments expressed in the reviews.\n",
    "- Readers would get a mostly inaccurate impression of users' opinions.\n",
    "\n",
    "<score>3</score> - OK (Partially Consistent, 51-70% consistent)\n",
    "\n",
    "- About half to two-thirds of the aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary partially represents the sentiments expressed in the reviews.\n",
    "- Readers would get a mixed impression of users' opinions, with some accuracy.\n",
    "\n",
    "<score>4</score> - Good (Mostly Consistent, 71-90% consistent)\n",
    "\n",
    "- Most aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely represents the sentiments expressed in the reviews, with minor inconsistencies.\n",
    "- Readers would get a mostly accurate impression of users' opinions.\n",
    "\n",
    "<score>5</score> - Excellent (Completely Consistent, 91-100% consistent)\n",
    "\n",
    "- All or nearly all aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary perfectly represents the sentiments expressed in the reviews.\n",
    "- Readers would get a fully accurate impression of users' opinions.\n",
    "\n",
    "\n",
    "Product Reviews: {{reviews}}\n",
    "\n",
    "Average Rating: {{average_rating}}\n",
    "\n",
    "Summary to Evaluate: {{Product_Opinion_Summary}}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Follow these steps strictly:\n",
    "\n",
    "Step 1. Identify all aspects of the product mentioned in the reviews and determine the majority sentiment for each aspect. List these with numbering.\n",
    "Step 2. Identify all aspects and their associated sentiments mentioned in the summary. List these with numbering.\n",
    "Step 3. Compare the aspects and sentiments from steps 1 and 2. Identify which aspects in the summary accurately reflect the majority sentiment from the reviews. List these with numbering.\n",
    "Step 4. Identify any aspects in the summary that misrepresent or fail to accurately convey the majority sentiment from the reviews. List these with numbering.\n",
    "Step 5. Calculate the percentage of aspects in the summary that accurately reflect the majority sentiment from the reviews.\n",
    "Step 6. Carefully match the observed sentiment consistency level to the descriptions in the evaluation criteria:\n",
    "\n",
    "   - Completely Inconsistent (0-20% consistent): Score 1 (Very Poor)\n",
    "   - Mostly Inconsistent (21-50% consistent): Score 2 (Poor)\n",
    "   - Partially Consistent (51-70% consistent): Score 3 (OK)\n",
    "   - Mostly Consistent (71-90% consistent): Score 4 (Good)\n",
    "   - Completely Consistent (91-100% consistent): Score 5 (Excellent)\n",
    "\n",
    "Step 7. Assign a score based on the sentiment consistency level, using the exact format shown in the evaluation criteria.\n",
    "\n",
    "Your response should follow this structure:\n",
    "\n",
    "1. Provide a detailed explanation of the sentiment consistency level of the summary, including specific examples of accurately and inaccurately represented sentiments for different aspects.\n",
    "2. Clearly state which sentiment consistency level this falls into, referencing the evaluation criteria.\n",
    "3. Assign a single score based on the sentiment consistency level, using the exact format shown below.\n",
    "\n",
    "Score format: Score- <score>X</score>\n",
    "Where X is the assigned score (1, 2, 3, 4, or 5) based on the sentiment consistency levels in the evaluation criteria.\n",
    "\n",
    "Remember, your final score must always be presented in this exact format, with no deviations. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_sentiment_consistency(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "    \n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_sentiment_consistency(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"l_sc_our_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4fcf6-3037-49ac-97c9-c9522c34ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"g.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "# System message and updated prompt template for sentiment consistency\n",
    "system_msg = \"\"\"You are a highly skilled expert in evaluating the sentiment consistency of product opinion summaries. Your expertise lies in analyzing summaries created from product reviews and ratings.\n",
    "\n",
    "Your primary responsibilities are:\n",
    "\n",
    "1. Carefully examine the provided product reviews and summary.\n",
    "2. Meticulously follow all instructions in the prompt faithfully and truthfully.\n",
    "3. Evaluate the summary's sentiment consistency with utmost accuracy and impartiality.\n",
    "4. Assign a single score (1-5) based on the quality of sentiment consistency, adhering strictly to the given evaluation criteria.\n",
    "5. Follow the specified format for all responses.\n",
    "\n",
    "Your expert evaluation is crucial for maintaining the accuracy of sentiment representation in product summaries. Approach each evaluation with diligence and attention to detail.\n",
    "\"\"\"\n",
    "\n",
    "sentiment_consistency_prompt_template = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Task Description: \n",
    "\n",
    "You will be given product reviews and an average rating. Next, you will be provided with one summary created using this product information. Your task is to carefully follow each evaluation criterion and instruction and always provide a faithful, truthful, and accurate output in the specified format. You must evaluate and assign a single score ranging from 1 to 5, to the summary, according to the metric called sentiment consistency.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "<score>1</score> - Very Poor (Completely Inconsistent, 0-20% consistent)\n",
    "\n",
    "- None of the aspects mentioned in the summary reflect the majority sentiment from the reviews.\n",
    "- The summary completely misrepresents the overall sentiment of the product.\n",
    "- Readers would get an entirely inaccurate impression of users' opinions.\n",
    "\n",
    "<score>2</score> - Poor (Mostly Inconsistent, 21-50% consistent)\n",
    "\n",
    "- Very few aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely misrepresents the sentiments expressed in the reviews.\n",
    "- Readers would get a mostly inaccurate impression of users' opinions.\n",
    "\n",
    "<score>3</score> - OK (Partially Consistent, 51-70% consistent)\n",
    "\n",
    "- About half to two-thirds of the aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary partially represents the sentiments expressed in the reviews.\n",
    "- Readers would get a mixed impression of users' opinions, with some accuracy.\n",
    "\n",
    "<score>4</score> - Good (Mostly Consistent, 71-90% consistent)\n",
    "\n",
    "- Most aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely represents the sentiments expressed in the reviews, with minor inconsistencies.\n",
    "- Readers would get a mostly accurate impression of users' opinions.\n",
    "\n",
    "<score>5</score> - Excellent (Completely Consistent, 91-100% consistent)\n",
    "\n",
    "- All or nearly all aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary perfectly represents the sentiments expressed in the reviews.\n",
    "- Readers would get a fully accurate impression of users' opinions.\n",
    "\n",
    "\n",
    "Product Reviews: {{reviews}}\n",
    "\n",
    "Average Rating: {{average_rating}}\n",
    "\n",
    "Summary to Evaluate: {{Product_Opinion_Summary}}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Follow these steps strictly:\n",
    "\n",
    "Step 1. Identify all aspects of the product mentioned in the reviews and determine the majority sentiment for each aspect. List these with numbering.\n",
    "Step 2. Identify all aspects and their associated sentiments mentioned in the summary. List these with numbering.\n",
    "Step 3. Compare the aspects and sentiments from steps 1 and 2. Identify which aspects in the summary accurately reflect the majority sentiment from the reviews. List these with numbering.\n",
    "Step 4. Identify any aspects in the summary that misrepresent or fail to accurately convey the majority sentiment from the reviews. List these with numbering.\n",
    "Step 5. Calculate the percentage of aspects in the summary that accurately reflect the majority sentiment from the reviews.\n",
    "Step 6. Carefully match the observed sentiment consistency level to the descriptions in the evaluation criteria:\n",
    "\n",
    "   - Completely Inconsistent (0-20% consistent): Score 1 (Very Poor)\n",
    "   - Mostly Inconsistent (21-50% consistent): Score 2 (Poor)\n",
    "   - Partially Consistent (51-70% consistent): Score 3 (OK)\n",
    "   - Mostly Consistent (71-90% consistent): Score 4 (Good)\n",
    "   - Completely Consistent (91-100% consistent): Score 5 (Excellent)\n",
    "\n",
    "Step 7. Assign a score based on the sentiment consistency level, using the exact format shown in the evaluation criteria.\n",
    "\n",
    "Your response should follow this structure:\n",
    "\n",
    "1. Provide a detailed explanation of the sentiment consistency level of the summary, including specific examples of accurately and inaccurately represented sentiments for different aspects.\n",
    "2. Clearly state which sentiment consistency level this falls into, referencing the evaluation criteria.\n",
    "3. Assign a single score based on the sentiment consistency level, using the exact format shown below.\n",
    "\n",
    "Score format: Score- <score>X</score>\n",
    "Where X is the assigned score (1, 2, 3, 4, or 5) based on the sentiment consistency levels in the evaluation criteria.\n",
    "\n",
    "Remember, your final score must always be presented in this exact format, with no deviations. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_sentiment_consistency(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "    \n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_sentiment_consistency(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"g_sc_our_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61476052-5d12-4c92-9238-192b18a69215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"v.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "# System message and updated prompt template for sentiment consistency\n",
    "system_msg = \"\"\"You are a highly skilled expert in evaluating the sentiment consistency of product opinion summaries. Your expertise lies in analyzing summaries created from product reviews and ratings.\n",
    "\n",
    "Your primary responsibilities are:\n",
    "\n",
    "1. Carefully examine the provided product reviews and summary.\n",
    "2. Meticulously follow all instructions in the prompt faithfully and truthfully.\n",
    "3. Evaluate the summary's sentiment consistency with utmost accuracy and impartiality.\n",
    "4. Assign a single score (1-5) based on the quality of sentiment consistency, adhering strictly to the given evaluation criteria.\n",
    "5. Follow the specified format for all responses.\n",
    "\n",
    "Your expert evaluation is crucial for maintaining the accuracy of sentiment representation in product summaries. Approach each evaluation with diligence and attention to detail.\n",
    "\"\"\"\n",
    "\n",
    "sentiment_consistency_prompt_template = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Task Description: \n",
    "\n",
    "You will be given product reviews and an average rating. Next, you will be provided with one summary created using this product information. Your task is to carefully follow each evaluation criterion and instruction and always provide a faithful, truthful, and accurate output in the specified format. You must evaluate and assign a single score ranging from 1 to 5, to the summary, according to the metric called sentiment consistency.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "<score>1</score> - Very Poor (Completely Inconsistent, 0-20% consistent)\n",
    "\n",
    "- None of the aspects mentioned in the summary reflect the majority sentiment from the reviews.\n",
    "- The summary completely misrepresents the overall sentiment of the product.\n",
    "- Readers would get an entirely inaccurate impression of users' opinions.\n",
    "\n",
    "<score>2</score> - Poor (Mostly Inconsistent, 21-50% consistent)\n",
    "\n",
    "- Very few aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely misrepresents the sentiments expressed in the reviews.\n",
    "- Readers would get a mostly inaccurate impression of users' opinions.\n",
    "\n",
    "<score>3</score> - OK (Partially Consistent, 51-70% consistent)\n",
    "\n",
    "- About half to two-thirds of the aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary partially represents the sentiments expressed in the reviews.\n",
    "- Readers would get a mixed impression of users' opinions, with some accuracy.\n",
    "\n",
    "<score>4</score> - Good (Mostly Consistent, 71-90% consistent)\n",
    "\n",
    "- Most aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely represents the sentiments expressed in the reviews, with minor inconsistencies.\n",
    "- Readers would get a mostly accurate impression of users' opinions.\n",
    "\n",
    "<score>5</score> - Excellent (Completely Consistent, 91-100% consistent)\n",
    "\n",
    "- All or nearly all aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary perfectly represents the sentiments expressed in the reviews.\n",
    "- Readers would get a fully accurate impression of users' opinions.\n",
    "\n",
    "\n",
    "Product Reviews: {{reviews}}\n",
    "\n",
    "Average Rating: {{average_rating}}\n",
    "\n",
    "Summary to Evaluate: {{Product_Opinion_Summary}}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Follow these steps strictly:\n",
    "\n",
    "Step 1. Identify all aspects of the product mentioned in the reviews and determine the majority sentiment for each aspect. List these with numbering.\n",
    "Step 2. Identify all aspects and their associated sentiments mentioned in the summary. List these with numbering.\n",
    "Step 3. Compare the aspects and sentiments from steps 1 and 2. Identify which aspects in the summary accurately reflect the majority sentiment from the reviews. List these with numbering.\n",
    "Step 4. Identify any aspects in the summary that misrepresent or fail to accurately convey the majority sentiment from the reviews. List these with numbering.\n",
    "Step 5. Calculate the percentage of aspects in the summary that accurately reflect the majority sentiment from the reviews.\n",
    "Step 6. Carefully match the observed sentiment consistency level to the descriptions in the evaluation criteria:\n",
    "\n",
    "   - Completely Inconsistent (0-20% consistent): Score 1 (Very Poor)\n",
    "   - Mostly Inconsistent (21-50% consistent): Score 2 (Poor)\n",
    "   - Partially Consistent (51-70% consistent): Score 3 (OK)\n",
    "   - Mostly Consistent (71-90% consistent): Score 4 (Good)\n",
    "   - Completely Consistent (91-100% consistent): Score 5 (Excellent)\n",
    "\n",
    "Step 7. Assign a score based on the sentiment consistency level, using the exact format shown in the evaluation criteria.\n",
    "\n",
    "Your response should follow this structure:\n",
    "\n",
    "1. Provide a detailed explanation of the sentiment consistency level of the summary, including specific examples of accurately and inaccurately represented sentiments for different aspects.\n",
    "2. Clearly state which sentiment consistency level this falls into, referencing the evaluation criteria.\n",
    "3. Assign a single score based on the sentiment consistency level, using the exact format shown below.\n",
    "\n",
    "Score format: Score- <score>X</score>\n",
    "Where X is the assigned score (1, 2, 3, 4, or 5) based on the sentiment consistency levels in the evaluation criteria.\n",
    "\n",
    "Remember, your final score must always be presented in this exact format, with no deviations. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_sentiment_consistency(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "    \n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_sentiment_consistency(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"v_sc_our_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeeece7-c74b-46d4-b82d-0897109458a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"z.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "# System message and updated prompt template for sentiment consistency\n",
    "system_msg = \"\"\"You are a highly skilled expert in evaluating the sentiment consistency of product opinion summaries. Your expertise lies in analyzing summaries created from product reviews and ratings.\n",
    "\n",
    "Your primary responsibilities are:\n",
    "\n",
    "1. Carefully examine the provided product reviews and summary.\n",
    "2. Meticulously follow all instructions in the prompt faithfully and truthfully.\n",
    "3. Evaluate the summary's sentiment consistency with utmost accuracy and impartiality.\n",
    "4. Assign a single score (1-5) based on the quality of sentiment consistency, adhering strictly to the given evaluation criteria.\n",
    "5. Follow the specified format for all responses.\n",
    "\n",
    "Your expert evaluation is crucial for maintaining the accuracy of sentiment representation in product summaries. Approach each evaluation with diligence and attention to detail.\n",
    "\"\"\"\n",
    "\n",
    "sentiment_consistency_prompt_template = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Task Description: \n",
    "\n",
    "You will be given product reviews and an average rating. Next, you will be provided with one summary created using this product information. Your task is to carefully follow each evaluation criterion and instruction and always provide a faithful, truthful, and accurate output in the specified format. You must evaluate and assign a single score ranging from 1 to 5, to the summary, according to the metric called sentiment consistency.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "<score>1</score> - Very Poor (Completely Inconsistent, 0-20% consistent)\n",
    "\n",
    "- None of the aspects mentioned in the summary reflect the majority sentiment from the reviews.\n",
    "- The summary completely misrepresents the overall sentiment of the product.\n",
    "- Readers would get an entirely inaccurate impression of users' opinions.\n",
    "\n",
    "<score>2</score> - Poor (Mostly Inconsistent, 21-50% consistent)\n",
    "\n",
    "- Very few aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely misrepresents the sentiments expressed in the reviews.\n",
    "- Readers would get a mostly inaccurate impression of users' opinions.\n",
    "\n",
    "<score>3</score> - OK (Partially Consistent, 51-70% consistent)\n",
    "\n",
    "- About half to two-thirds of the aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary partially represents the sentiments expressed in the reviews.\n",
    "- Readers would get a mixed impression of users' opinions, with some accuracy.\n",
    "\n",
    "<score>4</score> - Good (Mostly Consistent, 71-90% consistent)\n",
    "\n",
    "- Most aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely represents the sentiments expressed in the reviews, with minor inconsistencies.\n",
    "- Readers would get a mostly accurate impression of users' opinions.\n",
    "\n",
    "<score>5</score> - Excellent (Completely Consistent, 91-100% consistent)\n",
    "\n",
    "- All or nearly all aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary perfectly represents the sentiments expressed in the reviews.\n",
    "- Readers would get a fully accurate impression of users' opinions.\n",
    "\n",
    "\n",
    "Product Reviews: {{reviews}}\n",
    "\n",
    "Average Rating: {{average_rating}}\n",
    "\n",
    "Summary to Evaluate: {{Product_Opinion_Summary}}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Follow these steps strictly:\n",
    "\n",
    "Step 1. Identify all aspects of the product mentioned in the reviews and determine the majority sentiment for each aspect. List these with numbering.\n",
    "Step 2. Identify all aspects and their associated sentiments mentioned in the summary. List these with numbering.\n",
    "Step 3. Compare the aspects and sentiments from steps 1 and 2. Identify which aspects in the summary accurately reflect the majority sentiment from the reviews. List these with numbering.\n",
    "Step 4. Identify any aspects in the summary that misrepresent or fail to accurately convey the majority sentiment from the reviews. List these with numbering.\n",
    "Step 5. Calculate the percentage of aspects in the summary that accurately reflect the majority sentiment from the reviews.\n",
    "Step 6. Carefully match the observed sentiment consistency level to the descriptions in the evaluation criteria:\n",
    "\n",
    "   - Completely Inconsistent (0-20% consistent): Score 1 (Very Poor)\n",
    "   - Mostly Inconsistent (21-50% consistent): Score 2 (Poor)\n",
    "   - Partially Consistent (51-70% consistent): Score 3 (OK)\n",
    "   - Mostly Consistent (71-90% consistent): Score 4 (Good)\n",
    "   - Completely Consistent (91-100% consistent): Score 5 (Excellent)\n",
    "\n",
    "Step 7. Assign a score based on the sentiment consistency level, using the exact format shown in the evaluation criteria.\n",
    "\n",
    "Your response should follow this structure:\n",
    "\n",
    "1. Provide a detailed explanation of the sentiment consistency level of the summary, including specific examples of accurately and inaccurately represented sentiments for different aspects.\n",
    "2. Clearly state which sentiment consistency level this falls into, referencing the evaluation criteria.\n",
    "3. Assign a single score based on the sentiment consistency level, using the exact format shown below.\n",
    "\n",
    "Score format: Score- <score>X</score>\n",
    "Where X is the assigned score (1, 2, 3, 4, or 5) based on the sentiment consistency levels in the evaluation criteria.\n",
    "\n",
    "Remember, your final score must always be presented in this exact format, with no deviations. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_sentiment_consistency(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "    \n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_sentiment_consistency(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"z_sc_our_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae91f3-55cf-4914-abb3-798b8ebded05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file containing the product information and opinion summaries\n",
    "with open(\"4o.json\", \"r\") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "# System message and updated prompt template for sentiment consistency\n",
    "system_msg = \"\"\"You are a highly skilled expert in evaluating the sentiment consistency of product opinion summaries. Your expertise lies in analyzing summaries created from product reviews and ratings.\n",
    "\n",
    "Your primary responsibilities are:\n",
    "\n",
    "1. Carefully examine the provided product reviews and summary.\n",
    "2. Meticulously follow all instructions in the prompt faithfully and truthfully.\n",
    "3. Evaluate the summary's sentiment consistency with utmost accuracy and impartiality.\n",
    "4. Assign a single score (1-5) based on the quality of sentiment consistency, adhering strictly to the given evaluation criteria.\n",
    "5. Follow the specified format for all responses.\n",
    "\n",
    "Your expert evaluation is crucial for maintaining the accuracy of sentiment representation in product summaries. Approach each evaluation with diligence and attention to detail.\n",
    "\"\"\"\n",
    "\n",
    "sentiment_consistency_prompt_template = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Task Description: \n",
    "\n",
    "You will be given product reviews and an average rating. Next, you will be provided with one summary created using this product information. Your task is to carefully follow each evaluation criterion and instruction and always provide a faithful, truthful, and accurate output in the specified format. You must evaluate and assign a single score ranging from 1 to 5, to the summary, according to the metric called sentiment consistency.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Sentiment Consistency - Sentiment Consistency measures how accurately the summary reflects the consensus sentiment of users for each aspect of the product as expressed in the reviews. The consensus sentiment (or majority sentiment) for an aspect is determined by the most common sentiment expressed by users, categorized as very positive, positive, neutral, negative, or very negative.\n",
    "\n",
    "<score>1</score> - Very Poor (Completely Inconsistent, 0-20% consistent)\n",
    "\n",
    "- None of the aspects mentioned in the summary reflect the majority sentiment from the reviews.\n",
    "- The summary completely misrepresents the overall sentiment of the product.\n",
    "- Readers would get an entirely inaccurate impression of users' opinions.\n",
    "\n",
    "<score>2</score> - Poor (Mostly Inconsistent, 21-50% consistent)\n",
    "\n",
    "- Very few aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely misrepresents the sentiments expressed in the reviews.\n",
    "- Readers would get a mostly inaccurate impression of users' opinions.\n",
    "\n",
    "<score>3</score> - OK (Partially Consistent, 51-70% consistent)\n",
    "\n",
    "- About half to two-thirds of the aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary partially represents the sentiments expressed in the reviews.\n",
    "- Readers would get a mixed impression of users' opinions, with some accuracy.\n",
    "\n",
    "<score>4</score> - Good (Mostly Consistent, 71-90% consistent)\n",
    "\n",
    "- Most aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary largely represents the sentiments expressed in the reviews, with minor inconsistencies.\n",
    "- Readers would get a mostly accurate impression of users' opinions.\n",
    "\n",
    "<score>5</score> - Excellent (Completely Consistent, 91-100% consistent)\n",
    "\n",
    "- All or nearly all aspects in the summary accurately reflect the majority sentiment from the reviews.\n",
    "- The summary perfectly represents the sentiments expressed in the reviews.\n",
    "- Readers would get a fully accurate impression of users' opinions.\n",
    "\n",
    "\n",
    "Product Reviews: {{reviews}}\n",
    "\n",
    "Average Rating: {{average_rating}}\n",
    "\n",
    "Summary to Evaluate: {{Product_Opinion_Summary}}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Follow these steps strictly:\n",
    "\n",
    "Step 1. Identify all aspects of the product mentioned in the reviews and determine the majority sentiment for each aspect. List these with numbering.\n",
    "Step 2. Identify all aspects and their associated sentiments mentioned in the summary. List these with numbering.\n",
    "Step 3. Compare the aspects and sentiments from steps 1 and 2. Identify which aspects in the summary accurately reflect the majority sentiment from the reviews. List these with numbering.\n",
    "Step 4. Identify any aspects in the summary that misrepresent or fail to accurately convey the majority sentiment from the reviews. List these with numbering.\n",
    "Step 5. Calculate the percentage of aspects in the summary that accurately reflect the majority sentiment from the reviews.\n",
    "Step 6. Carefully match the observed sentiment consistency level to the descriptions in the evaluation criteria:\n",
    "\n",
    "   - Completely Inconsistent (0-20% consistent): Score 1 (Very Poor)\n",
    "   - Mostly Inconsistent (21-50% consistent): Score 2 (Poor)\n",
    "   - Partially Consistent (51-70% consistent): Score 3 (OK)\n",
    "   - Mostly Consistent (71-90% consistent): Score 4 (Good)\n",
    "   - Completely Consistent (91-100% consistent): Score 5 (Excellent)\n",
    "\n",
    "Step 7. Assign a score based on the sentiment consistency level, using the exact format shown in the evaluation criteria.\n",
    "\n",
    "Your response should follow this structure:\n",
    "\n",
    "1. Provide a detailed explanation of the sentiment consistency level of the summary, including specific examples of accurately and inaccurately represented sentiments for different aspects.\n",
    "2. Clearly state which sentiment consistency level this falls into, referencing the evaluation criteria.\n",
    "3. Assign a single score based on the sentiment consistency level, using the exact format shown below.\n",
    "\n",
    "Score format: Score- <score>X</score>\n",
    "Where X is the assigned score (1, 2, 3, 4, or 5) based on the sentiment consistency levels in the evaluation criteria.\n",
    "\n",
    "Remember, your final score must always be presented in this exact format, with no deviations. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''\n",
    "\n",
    "# Function to evaluate a single opinion summary\n",
    "def evaluate_sentiment_consistency(product):\n",
    "    reviews = \"\\n\".join(product.get(\"reviews\", [])) if product.get(\"reviews\") else \"N/A\"\n",
    "    average_rating = product.get(\"averageRating\", \"N/A\")\n",
    "    opinion_summary = product.get(\"product_opinion_summary\", \"\")\n",
    "    \n",
    "    prompt = sentiment_consistency_prompt_template.format(\n",
    "        reviews=reviews,\n",
    "        average_rating=average_rating,\n",
    "        Product_Opinion_Summary=opinion_summary\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Set up sampling parameters\n",
    "\n",
    "\n",
    "\n",
    "# Process products in batches\n",
    "batch_size = 5\n",
    "evaluation_results = []\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(products), batch_size):\n",
    "    batch_products = products[i:i+batch_size]\n",
    "    \n",
    "    # Prepare prompts for the current batch\n",
    "    prompts = [evaluate_sentiment_consistency(product) for product in batch_products]\n",
    "    \n",
    "    # Generate outputs using vLLM\n",
    "    outputs = model.generate(prompts, sampling_params)\n",
    "    \n",
    "    # Process the outputs\n",
    "    for j, output_group in enumerate(outputs):\n",
    "        product_title = batch_products[j].get(\"product_title\", \"\")\n",
    "        for k, output in enumerate(output_group.outputs):\n",
    "            generated_text = output.text\n",
    "            evaluation_results.append(f\"**{product_title} - Evaluation {k + 1}**\\n\\nResponse: {generated_text}\\n\")\n",
    "    \n",
    "    print(f\"Processed batch {i // batch_size + 1} of {(len(products) - 1) // batch_size + 1}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open(\"4o_sc_our_dep.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(evaluation_results))\n",
    "\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
